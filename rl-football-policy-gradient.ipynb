{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# GFootball environment.\n!pip install kaggle_environments\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!git clone -b v2.3 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n# Some helper code\n!git clone https://github.com/garethjns/kaggle-football.git\n!pip install reinforcement_learning_keras==0.6.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nfrom typing import Union, Callable, List, Tuple, Iterable, Any, Dict\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow import keras\nimport tensorflow as tf\nimport seaborn as sns\nimport gym\nimport gfootball\nimport glob \nimport imageio\nimport pathlib\nimport zlib\nimport pickle\nimport tempfile\nimport os\nimport sys\nfrom IPython.display import Image, display\nfrom gfootball.env import observation_preprocessing\nsns.set()\n\n# In TF > 2, training keras models in a loop with eager execution on causes memory leaks and terrible performance.\ntf.compat.v1.disable_eager_execution()\n\nsys.path.append(\"/kaggle/working/kaggle-football/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division\nfrom __future__ import print_function\nimport itertools as it\nfrom random import sample, randint, random\nfrom time import time, sleep\nimport numpy as np\nimport skimage.color, skimage.transform\nimport tensorflow as tf\nfrom tqdm import trange\nfrom argparse import ArgumentParser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"football = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\nprint(env.reset().shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gym\nimport numpy as np\n\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras import backend as K\n\nfrom collections import deque\n\ndef one_hot(index, categories):\n    x = np.zeros((categories,))\n    x[index] = 1\n    return x\n\ndef discount_rewards(r, gamma=0.99):\n    \"\"\" Take 1D float array of rewards and compute discounted reward \"\"\"\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    for t in reversed(range(0, len(r))):\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r\n\ndef pg_loss(advantage):\n    def f(y_true, y_pred):\n        \"\"\"\n        Policy gradient loss\n        \"\"\"\n        # L = \\sum{A * log(p(y | x))}\n        # Mask out probability of action taken\n        responsible_outputs = K.sum(y_true * y_pred, axis=1)\n        policy_loss = -K.sum(advantage * K.log(responsible_outputs))\n        return policy_loss\n    return f\n\ndef create_model():\n    \"\"\"\n    Model architecture\n    \"\"\"\n    state = Input(shape=(72,96,4))\n    x = Conv2D(32 , kernel_size= 3 , strides=(2,2) , padding=\"valid\")(state)\n    x = Conv2D(64 , kernel_size= 3 , strides=(2,2) , padding=\"valid\")(x)\n    x = Conv2D(128 , kernel_size= 3 , strides=(2,2) , padding=\"valid\")(x)\n    x = Flatten()(x)\n    x = Dense(256 , activation=\"relu\")(x)\n    x = Dense(19)(x)\n    x = Activation('softmax')(x)\n\n    model = Model(state, x)\n    return model\n\ndef pg(model):\n    \"\"\"\n    Wraps the model in a policy gradient model\n    \"\"\"\n    state = Input(shape=(72,96,4))\n    # Advantages for loss function\n    adv_input = Input(shape=(1,))\n\n    x = model(state)\n\n    model = Model([state, adv_input], x)\n    model.compile(\n        optimizer='nadam',\n        loss=pg_loss(adv_input)\n    )\n\n    return model\n\ng_model = create_model()\npg_model = pg(g_model)\nall_rewards = deque(maxlen=100)\n\nfor i_episode in range(100):\n    observation = football.reset()\n\n    # History of this episode\n    state_history = []\n    action_history = []\n    reward_history = []\n\n    for t in range(1000):\n        # env.render()\n\n        state_history.append(observation)\n\n        action_prob = g_model.predict(np.expand_dims(observation, axis=0))[0]\n        action = np.random.choice(len(action_prob), 1, p=action_prob)[0]\n        observation, reward, done, info = football.step(action)\n\n        reward_history.append(reward)\n        action_history.append(one_hot(action, 19))\n\n        if done:\n            reward_sum = sum(reward_history)\n            all_rewards.append(reward_sum)\n\n            adv = discount_rewards(reward_history)\n\n            state_history = np.array(state_history)\n            action_history = np.array(action_history)\n\n            pg_model.train_on_batch([state_history, adv], action_history)\n\n            print(\"Episode finished with reward {} {:.2f}\".format(reward_sum, np.mean(all_rewards)))\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_model.save(\"deep_policy_gradient.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile main.py\n\nimport collections\nimport pickle\nimport zlib\nfrom typing import Tuple, Dict, Any, Union, Callable, List\n\nimport gym\nfrom gfootball.env import observation_preprocessing\nfrom tensorflow import keras\n\nimport random                # Handling random number generation\nimport time                  # Handling time calculation\nfrom skimage import transform# Help us to preprocess the frames\n\nfrom collections import deque# Ordered collection with ends\nfrom collections import namedtuple\nimport numpy as np\n\nfrom keras.models import load_model\nimport numpy as np\nimport keras.backend as K\nimport tensorflow as tf\n\n\ntry:\n    model_policy_grad = load_model('/kaggle_simulations/agent/deep_policy_gradient.h5')\nexcept (FileNotFoundError, ValueError):\n    model_policy_grad = load_model('deep_policy_gradient.h5')\n\ndef agent(obs):\n\n    # Get the raw observations return by the environment\n    obs = obs['players_raw'][0]\n    # Convert these to the same output as the SMMWrapper we used in training\n    obs = observation_preprocessing.generate_smm([obs]).squeeze()\n    \n    #inference the model\n    action_probs = model_policy_grad.predict(np.expand_dims(state , 0 ) , steps=1)\n    # Use the SMMFrameProcessWrapper to do the buffering, but not enviroment\n    # stepping or anything related to the Gym API.\n    action = np.argmax(action_probs)\n\n    return [int(action)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Tuple, Dict, List, Any\n\nfrom kaggle_environments import make\n\nenv = make(\"football\", debug=True,configuration={\"save_video\": True,\n                                      \"scenario_name\": \"11_vs_11_kaggle\"})\n\n# Define players\nleft_player = \"/kaggle/working/main.py\"  # A custom agent, eg. random_agent.py or example_agent.py\nright_player = \"run_right\"  # eg. A built in 'AI' agent or the agent again\n\n\noutput: List[Tuple[Dict[str, Any], Dict[str, Any]]] = env.run([left_player, right_player])\n\n#print(f\"Final score: {sum([r['reward'] for r in output[0]])} : {sum([r['reward'] for r in output[1]])}\")\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!tar -czvf submission.tar.gz ./main.py*  ./dqn_keras.h5*","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}