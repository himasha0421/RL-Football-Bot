{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Actor Critic Method"},{"metadata":{},"cell_type":"markdown","source":"## Setup the enviroment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GFootball environment.\n!pip install kaggle_environments\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!git clone -b v2.3 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n# Some helper code\n!git clone https://github.com/garethjns/kaggle-football.git\n!pip install reinforcement_learning_keras==0.6.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division\nfrom __future__ import print_function\nimport collections\nfrom typing import Union, Callable, List, Tuple, Iterable, Any, Dict\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow import keras\nimport tensorflow as tf\nimport seaborn as sns\nimport gym\nimport gfootball\nimport glob \nimport imageio\nimport pathlib\nimport zlib\nimport pickle\nimport tempfile\nimport os\nimport sys\nfrom IPython.display import Image, display\nfrom gfootball.env import observation_preprocessing\nsns.set()\n\n# In TF > 2, training keras models in a loop with eager execution on causes memory leaks and terrible performance.\ntf.compat.v1.disable_eager_execution()\n\nsys.path.append(\"/kaggle/working/kaggle-football/\")\n\nimport itertools as it\nfrom random import sample, randint, random\nfrom time import time, sleep\nimport numpy as np\nimport skimage.color, skimage.transform\nimport tensorflow as tf\nfrom tqdm import trange\nfrom argparse import ArgumentParser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random                # Handling random number generation\nimport time                  # Handling time calculation\nfrom skimage import transform# Help us to preprocess the frames\n\nfrom collections import deque# Ordered collection with ends\nfrom collections import namedtuple\nimport torch\nimport torch.optim as optim\nimport numpy as np\nstacked_size = 3\n\nclass DataPreprocess():\n    \n    def __init__(self , stack_size ):\n        self.stack_size = stack_size\n        self.stacked_frames = deque([ np.zeros([4,72,96] ,dtype=int) for i_dx in range(stacked_size) ] , maxlen=self.stack_size)\n\n    def preprocess(self , frame):\n        \"\"\"\n        screen frames are in grayscale format defalut\n        1.normalize the images\n        2. apply some transformations\n        \"\"\"\n        frame = frame / 255.0\n        frame = transform.resize(frame ,[4,72,96])\n\n        return frame\n\n    def reset(self ):\n        \n        self.stacked_frames = deque([ np.zeros([4,72,96] ,dtype=int) for i_dx in range(stacked_size) ] , maxlen=self.stack_size)\n        \n    def stack_frames(self , frame , new_episode=False):\n        \"\"\"\n        stack multiple frames with each other to idenitify the temporal movemnts of the objects\n        1. preprocess the new frame\n        \"\"\"\n        processed_frame = self.preprocess(frame)\n        if(new_episode):\n            #for initial step after new episode add same frame to all the stacked frames\n            self.reset()\n            self.stacked_frames.append(processed_frame)\n            self.stacked_frames.append(processed_frame)\n\n            stack_states = np.concatenate(self.stacked_frames , axis=0)\n\n            return stack_states\n\n        else:\n            self.stacked_frames.append(processed_frame)\n\n            stack_states = np.concatenate(self.stacked_frames , axis=0)\n\n            return stack_states","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef hidden_init(layer):\n    fan_in = layer.weight.data.size()[0]\n    lim = 1. / np.sqrt(fan_in)\n    return (-lim, lim)\n\nclass Actor(nn.Module):\n    \"\"\"Actor (Policy) Model.\"\"\"\n\n    def __init__(self,stack_size , action_size , seed):\n        super(Actor, self).__init__()\n        \"\"\"\n        define a simple model with some conv layers and later with fully connected layers\n        \"\"\"\n        self.in_size = stack_size\n        self.out_size = action_size\n        self.seed = torch.manual_seed(seed)\n        self.conv_block1 = nn.Conv2d(self.in_size , 32 ,    kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.conv_block2 = nn.Conv2d(32 , 128 ,  kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.conv_block3 = nn.Conv2d(128 , 512 , kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.conv_block4 = nn.Conv2d(512 , 1024 ,kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.flatten_size = 1024*5*6\n        self.fc1 = nn.Linear(self.flatten_size , 256)\n        self.fc_bn = nn.BatchNorm1d(256)\n        self.fc2 = nn.Linear(256 , self.out_size)\n        \n    def forward(self, x):\n\n        #layer 1 infernce\n        x = self.conv_block1(x)\n        # layer 2 inference\n        x = self.conv_block2(x)\n        # layer 3 infernce\n        x = self.conv_block3(x)\n        # layer 4 infernce\n        x = self.conv_block4(x)\n        x = x.view(-1,self.flatten_size)\n        x = F.dropout(F.relu(self.fc_bn(self.fc1(x))) ,p=0.4 )\n        # model output\n        x = self.fc2(x) \n        \n        return x\n\nclass Critic(nn.Module):\n    \"\"\"Critic (Value) Model.\"\"\"\n\n    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n        \"\"\"Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fcs1_units (int): Number of nodes in the first hidden layer\n            fc2_units (int): Number of nodes in the second hidden layer\n        \"\"\"\n        super(Critic, self).__init__()\n        \n        self.seed = torch.manual_seed(seed)\n        \n        self.in_size = state_size\n        self.out_size = action_size\n        self.seed = torch.manual_seed(seed)\n        \n        self.conv_block1 = nn.Conv2d(self.in_size ,  32 ,  kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.conv_block2 = nn.Conv2d(32 , 64 ,  kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.conv_block3 = nn.Conv2d(64 , 128 , kernel_size=3 , stride=2 , padding=1 , bias=False)\n        \n        self.flatten_size = 128*9*12\n        self.fc1 = nn.Linear(self.flatten_size , fcs1_units)\n        \n        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n        self.fc3 = nn.Linear(fc2_units, 1)\n\n    def forward(self, state, action):\n        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n        \n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.conv_block3(x)\n        x = x.view(-1,self.flatten_size)\n        xs = F.dropout(F.relu(self.fc1(x)) ,p=0.4 )\n        \n        x = torch.cat((xs, action), dim=1)\n        x = F.relu(self.fc2(x))\n        \n        return self.fc3(x)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport copy\nfrom collections import namedtuple, deque\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nBUFFER_SIZE = int(1e5)  # replay buffer size\nBATCH_SIZE = 128        # minibatch size\nGAMMA = 0.99            # discount factor\nTAU = 1e-3              # for soft update of target parameters\nLR_ACTOR = 1e-4         # learning rate of the actor \nLR_CRITIC = 1e-3        # learning rate of the critic\nWEIGHT_DECAY = 0        # L2 weight decay\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n\nclass Agent():\n    \"\"\"Interacts with and learns from the environment.\"\"\"\n    \n    def __init__(self, state_size, action_size, random_seed):\n        \"\"\"Initialize an Agent object.\n        \n        Params\n        ======\n            state_size (int): dimension of each state\n            action_size (int): dimension of each action\n            random_seed (int): random seed\n        \"\"\"\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(random_seed)\n\n        # Actor Network (w/ Target Network)\n        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n\n        # Critic Network (w/ Target Network)\n        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n\n        # Noise process\n        self.noise = OUNoise(action_size, random_seed)\n\n        # Replay memory\n        self.memory = ReplayBuffer( BUFFER_SIZE, BATCH_SIZE, random_seed)\n    \n    def step(self, state, action, reward, next_state, done):\n        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n        # Save experience / reward\n        self.memory.add(state, action, reward, next_state, done)\n\n        # Learn, if enough samples are available in memory\n        if len(self.memory) > BATCH_SIZE:\n            experiences = self.memory.sample()\n            self.learn(experiences, GAMMA)\n\n    def act(self, state, add_noise=True):\n        \"\"\"Returns actions for given state as per current policy.\"\"\"\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        self.actor_local.eval()\n        with torch.no_grad():\n            action = self.actor_local(state).cpu().data.numpy()\n        self.actor_local.train()\n        if add_noise:\n            action += self.noise.sample()\n        \n        return np.argmax(action)\n\n    def reset(self):\n        self.noise.reset()\n\n    def learn(self, experiences, gamma):\n        \"\"\"Update policy and value parameters using given batch of experience tuples.\n        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n        where:\n            actor_target(state) -> action\n            critic_target(state, action) -> Q-value\n\n        Params\n        ======\n            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n            gamma (float): discount factor\n        \"\"\"\n        states, actions, rewards, next_states, dones = experiences\n\n        # ---------------------------- update critic ---------------------------- #\n        # Get predicted next-state actions and Q values from target models\n        actions_next = self.actor_target(next_states)\n        Q_targets_next = self.critic_target(next_states, actions_next)\n        # Compute Q targets for current states (y_i)\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n        # Compute critic loss\n        Q_expected = self.critic_local(states, actions)\n        critic_loss = F.mse_loss(Q_expected, Q_targets)\n        # Minimize the loss\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # ---------------------------- update actor ---------------------------- #\n        # Compute actor loss\n        actions_pred = self.actor_local(states)\n        actor_loss = -self.critic_local(states, actions_pred).mean()\n        # Minimize the loss\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # ----------------------- update target networks ----------------------- #\n        self.soft_update(self.critic_local, self.critic_target, TAU)\n        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n\n    def soft_update(self, local_model, target_model, tau):\n        \"\"\"Soft update model parameters.\n        θ_target = τ*θ_local + (1 - τ)*θ_target\n\n        Params\n        ======\n            local_model: PyTorch model (weights will be copied from)\n            target_model: PyTorch model (weights will be copied to)\n            tau (float): interpolation parameter \n        \"\"\"\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n\nclass OUNoise:\n    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n\n    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n        \"\"\"Initialize parameters and noise process.\"\"\"\n        self.mu = mu * np.ones(size)\n        self.theta = theta\n        self.sigma = sigma\n        self.seed = random.seed(seed)\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n        self.state = copy.copy(self.mu)\n\n    def sample(self):\n        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n        x = self.state\n        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n        self.state = x + dx\n        return self.state\n\n\nclass ReplayBuffer :\n    def __init__(self , batch_size , buffer_size , seed):\n        self.batch_size = batch_size \n        self.buffer_size = buffer_size \n        self.seed = seed\n        random.seed(self.seed)\n        self.memory = deque(maxlen=self.buffer_size)\n        self.experience = namedtuple('Experience' , field_names=['state','action','reward','next_state','done'])\n        \n    def add(self , state , action , reward , next_state , done):\n        experience = self.experience(state , action , reward , next_state , done)\n        self.memory.append(experience)\n        \n    def sample(self):\n        \n        experinece_batch = random.sample(self.memory , k=self.batch_size)\n        \n        states = torch.from_numpy( np.stack([ e.state.reshape(8,72,96) for e in experinece_batch if e is not None ],axis=0) ).float().to(device)\n        action = torch.from_numpy( np.stack([ (e.action).reshape(1,) for e in experinece_batch if e is not None] , axis=0) ).long().to(device)\n        reward = torch.from_numpy( np.stack([ np.array((e.reward)).reshape(1,) for e in experinece_batch if e is not None] , axis=0) ).float().to(device)\n        next_state = torch.from_numpy( np.stack([ e.next_state.reshape(8,72,96) for e in experinece_batch if e is not None ] , axis=0) ).float().to(device)\n        done = torch.from_numpy( np.stack([ np.array((e.done)).reshape(1,) for e in experinece_batch if e is not None], axis=0).astype(np.uint8) ).float().to(device)\n        \n        return ( states , action , reward , next_state , done )\n    \n    def __len__(self):\n        return len(self.memory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"football_env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agent = Agent(state_size=8, action_size=19, random_seed=10)\ndata_processor = DataPreprocess(stack_size=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ddpg(n_episodes=200, max_t=1000):\n    scores_deque = deque(maxlen=100)\n    scores = []\n    max_score = -np.Inf\n    for i_episode in range(1, n_episodes+1):\n        state = football_env.reset()\n        # Remember that stack frame function also call our preprocess function.\n        state  = data_processor.stack_frames(state , new_episode=True)\n        agent.reset()\n        score = 0\n        for t in range(max_t):\n            action = agent.act(state)\n            next_state, reward, done, _ = football_env.step(action)\n            #prepare the next frame\n            next_state = data_processor.stack_frames( next_state, False)\n            #set the agent with the new data\n            agent.step(state, action, reward, next_state, done)\n            state = next_state\n            score += reward\n            if done:\n                break \n        scores_deque.append(score)\n        scores.append(score)\n        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n        if i_episode % 100 == 0:\n            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n    return scores\n\nscores = ddpg()\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.plot(np.arange(1, len(scores)+1), scores)\nplt.ylabel('Score')\nplt.xlabel('Episode #')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(agent.actor_local.state_dict() , \"checkpoint_actor.pth\")\ntorch.save(agent.critic_local.state_dict() , \"checkpoint_actor.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile main.py\n\n#from kaggle_environments.envs.football.helpers import *\n\n# @human_readable_agent wrapper modifies raw observations \n# provided by the environment:\n# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations\n# into a form easier to work with by humans.\n# Following modifications are applied:\n# - Action, PlayerRole and GameMode enums are introduced.\n# - 'sticky_actions' are turned into a set of active actions (Action enum)\n#    see usage example below.\n# - 'game_mode' is turned into GameMode enum.\n# - 'designated' field is removed, as it always equals to 'active'\n#    when a single player is controlled on the team.\n# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.\n# - Action enum is to be returned by the agent function.\n\n\nimport collections\nimport pickle\nimport zlib\nfrom typing import Tuple, Dict, Any, Union, Callable, List\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torch.optim as optim\n\nimport gym\nimport numpy as np\nfrom gfootball.env import observation_preprocessing\n\nimport random                # Handling random number generation\nimport time                  # Handling time calculation\nfrom skimage import transform# Help us to preprocess the frames\n\nfrom collections import deque# Ordered collection with ends\nfrom collections import namedtuple\n\n\nclass Data_Processing():\n    \n    def __init__(self , stack_size ):\n        self.stack_size = stack_size\n        self.stacked_frames = deque([ np.zeros([4,72,96] ,dtype=int) for i_dx in range(stacked_size) ] , maxlen=self.stack_size)\n\n    def preprocess(self , frame):\n        \"\"\"\n        screen frames are in grayscale format defalut\n        1.normalize the images\n        2. apply some transformations\n        \"\"\"\n        frame = frame / 255.0\n        frame = transform.resize(frame ,[4,72,96])\n\n        return frame\n\n    def reset(self ):\n        \n        self.stacked_frames = deque([ np.zeros([4,72,96] ,dtype=int) for i_dx in range(stacked_size) ] , maxlen=self.stack_size)\n        \n    def stack_frames(self , frame , new_episode=False):\n        \"\"\"\n        stack multiple frames with each other to idenitify the temporal movemnts of the objects\n        1. preprocess the new frame\n        \"\"\"\n        processed_frame = self.preprocess(frame)\n        if(new_episode):\n            #for initial step after new episode add same frame to all the stacked frames\n            self.reset()\n            self.stacked_frames.append(processed_frame)\n            self.stacked_frames.append(processed_frame)\n\n            stack_states = np.concatenate(self.stacked_frames , axis=0)\n\n            return stack_states\n\n        else:\n            self.stacked_frames.append(processed_frame)\n\n            stack_states = np.concatenate(self.stacked_frames , axis=0)\n\n            return stack_states\n    \n\nclass Actor(nn.Module):\n    \"\"\"Actor (Policy) Model.\"\"\"\n\n    def __init__(self,stack_size , action_size , seed):\n        super(Actor, self).__init__()\n        \"\"\"\n        define a simple model with some conv layers and later with fully connected layers\n        \"\"\"\n        self.in_size = stack_size\n        self.out_size = action_size\n        self.seed = torch.manual_seed(seed)\n        self.conv_block1 = nn.Conv2d(self.in_size , 32 ,    kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.conv_block2 = nn.Conv2d(32 , 128 ,  kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.conv_block3 = nn.Conv2d(128 , 512 , kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.conv_block4 = nn.Conv2d(512 , 1024 ,kernel_size=3 , stride=2 , padding=1 , bias=False)\n        self.flatten_size = 1024*5*6\n        self.fc1 = nn.Linear(self.flatten_size , 256)\n        self.fc_bn = nn.BatchNorm1d(256)\n        self.fc2 = nn.Linear(256 , self.out_size)\n        \n    def forward(self, x):\n\n        #layer 1 infernce\n        x = self.conv_block1(x)\n        # layer 2 inference\n        x = self.conv_block2(x)\n        # layer 3 infernce\n        x = self.conv_block3(x)\n        # layer 4 infernce\n        x = self.conv_block4(x)\n        x = x.view(-1,self.flatten_size)\n        x = F.dropout(F.relu(self.fc_bn(self.fc1(x))) ,p=0.4 )\n        # model output\n        x = self.fc2(x) \n        \n        return x\n    \n    \nactor = Actor(8,19,2333)\ntry:\n    actor.load_state_dict(torch.load(\"/kaggle_simulations/agent/checkpoint_actor.pth\" , map_location=\"cpu\"))\n    \nexcept (FileNotFoundError, ValueError):\n    actor.load_state_dict(torch.load(\"checkpoint_actor.pth\" , map_location=\"cpu\")\n                          \nactor.eval()\n\n\ndata_buffer = Data_Processing(stack_size=2)\n\n#@human_readable_agent\ndef agent(obs):\n\n    # Get the raw observations return by the environment\n    obs = obs['players_raw'][0]\n    # Convert these to the same output as the SMMWrapper we used in training\n    obs = observation_preprocessing.generate_smm([obs]).squeeze()\n    \n    state = data_buffer.stack_frames(obs ,False)\n    \n    actor_state = torch.from_numpy(state).float().unsqueeze(0)\n    #inference the model\n    actor_action = actor.forward(actor_state)\n\n    # Use the SMMFrameProcessWrapper to do the buffering, but not enviroment\n    # stepping or anything related to the Gym API.\n    action = np.argmax(actor_action.to('cpu').detach().numpy()) \n\n    return [int(action)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Tuple, Dict, List, Any\n\nfrom kaggle_environments import make\n\nenv = make(\"football\", debug=True,configuration={\"save_video\": True,\n                                      \"scenario_name\": \"11_vs_11_kaggle\"})\n\n# Define players\nleft_player = \"/kaggle/working/main.py\"  # A custom agent, eg. random_agent.py or example_agent.py\nright_player = \"run_right\"  # eg. A built in 'AI' agent or the agent again\n\n\noutput: List[Tuple[Dict[str, Any], Dict[str, Any]]] = env.run([left_player, right_player])\n\n#print(f\"Final score: {sum([r['reward'] for r in output[0]])} : {sum([r['reward'] for r in output[1]])}\")\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!tar -czvf submission.tar.gz ./main.py*  ./checkpoint_actor.pth*","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}