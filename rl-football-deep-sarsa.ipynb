{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# GFootball environment.\n!pip install kaggle_environments\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!git clone -b v2.3 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n# Some helper code\n!git clone https://github.com/garethjns/kaggle-football.git\n!pip install reinforcement_learning_keras==0.6.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nfrom typing import Union, Callable, List, Tuple, Iterable, Any, Dict\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow import keras\nimport tensorflow as tf\nimport seaborn as sns\nimport gym\nimport gfootball\nimport glob \nimport imageio\nimport pathlib\nimport zlib\nimport pickle\nimport tempfile\nimport os\nimport sys\nfrom IPython.display import Image, display\nfrom gfootball.env import observation_preprocessing\nsns.set()\n\n# In TF > 2, training keras models in a loop with eager execution on causes memory leaks and terrible performance.\ntf.compat.v1.disable_eager_execution()\n\nsys.path.append(\"/kaggle/working/kaggle-football/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division\nfrom __future__ import print_function\nimport itertools as it\nfrom random import sample, randint, random\nfrom time import time, sleep\nimport numpy as np\nimport skimage.color, skimage.transform\nimport tensorflow as tf\nfrom tqdm import trange\nfrom argparse import ArgumentParser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataWrapper():\n\n    def __init__(self, obs_shape= (72, 96, 4)) :\n        \"\"\"\n        :param env: Gym env, or None. Allowing None here is unusual,\n                    but we'll reuse the buffer functunality later in\n                    the submission, when we won't be using the gym API.\n        :param obs_shape: Expected shape of single observation.\n        \"\"\"\n        self.data_length = 2\n        self.frame_shape = obs_shape\n        self.init_frame_buffer()\n\n    @staticmethod\n    def normalize_frame(frame):\n        return frame / 255.0\n\n    def init_frame_buffer(self):\n        \"\"\"Create buffer and preallocate with empty arrays of expected shape.\"\"\"\n\n        self.frame_buffer = collections.deque(maxlen=self.data_length)\n\n        for _ in range(self.data_length):\n            self.frame_buffer.append(np.zeros(shape=self.frame_shape))\n\n    def build_buffered_frames(self):\n        \"\"\"\n        Iterate over the last dimenion, and take the difference between this obs \n        and the last obs for each.\n        \"\"\"\n        diff_buff = np.empty(self.frame_shape)\n        for f in range(self.frame_shape[-1]):\n            diff_buff[..., f] = self.frame_buffer[1][..., f] - self.frame_buffer[0][..., f]\n\n        return diff_buff\n\n    def stack_buffer(self, frame ):\n\n        frame = self.normalize_frame(frame)\n        self.frame_buffer.append(frame)\n\n        return self.build_buffered_frames()\n\n    def reset(self , reset_frame):\n        \"\"\"Add initial obs to end of pre-allocated buffer.\n\n        :return: Buffered observation\n        \"\"\"\n        self.init_frame_buffer()\n        self.frame_buffer.append(self.normalize_frame(reset_frame))\n\n        return self.build_buffered_frames()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import copy\nimport pylab\nimport random\nimport numpy as np\nfrom keras.layers import Dense , Conv2D , Flatten\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\n\nEPISODES = 10\n\n\n# this is DeepSARSA Agent for the GridWorld\n# Utilize Neural Network as q function approximator\nclass DeepSARSAgent:\n    def __init__(self):\n        self.load_model = False\n        # actions which agent can do\n        self.action_space = list(range(19))\n        # get size of state and action\n        self.action_size = len(self.action_space)\n        self.state_size = 4\n        self.discount_factor = 0.99\n        self.learning_rate = 0.001\n\n        self.epsilon = 1.  # exploration\n        self.epsilon_decay = .999999\n        self.epsilon_min = 0.01\n        self.model = self.build_model()\n\n        if self.load_model:\n            self.epsilon = 0.05\n            self.model.load_weights('./save_model/deep_sarsa_trained.h5')\n\n    # approximate Q function using Neural Network\n    # state is input and Q Value of each action is output of network\n    def build_model(self):\n        \n        model   = Sequential()\n        input_shape = (72, 96, 4)\n        model.add(Conv2D(32 , kernel_size= 3 , strides=(2,2) ,  input_shape=input_shape , padding=\"valid\"))\n        model.add(Conv2D(64 , kernel_size= 5 , strides=(2,2) , padding=\"valid\"))\n        model.add(Conv2D(128 , kernel_size= 5 , strides=(2,2) , padding=\"valid\"))\n        model.add(Flatten())\n        model.add(Dense(256 , activation=\"relu\"))\n        model.add(Dense(19))\n        model.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=self.learning_rate))\n        model.summary()\n        return model\n\n    # get action from model using epsilon-greedy policy\n    def get_action(self, state):\n        if np.random.rand() <= self.epsilon:\n            # The agent acts randomly\n            return random.randrange(self.action_size)\n        else:\n            # Predict the reward value based on the given state\n            state = np.expand_dims(np.float32(state),0)\n            q_values = self.model.predict(state)\n            return np.argmax(q_values[0])\n\n    def train_model(self, state, action, reward, next_state, next_action, done):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n        state = np.expand_dims(np.float32(state),0)\n        next_state = np.expand_dims( np.float32(next_state) ,0 )\n        target = self.model.predict(state   )[0]\n        # like Q Learning, get maximum Q value at s'\n        # But from target model\n        if done:\n            target[action] = reward\n        else:\n            target[action] = (reward + self.discount_factor *\n                              self.model.predict( next_state )[0][next_action])\n\n        target = np.reshape(target, [1, 19])\n        # make minibatch which includes target q value and predicted q value\n        # and do the model fit!\n        self.model.fit(state, target, epochs=1, verbose=0)\n\n\nif __name__ == \"__main__\":\n    \n    football_env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\n    agent = DeepSARSAgent()\n    data_processor = DataWrapper()\n    \n    global_step = 0\n    scores, episodes = [], []\n\n    for e in range(EPISODES):\n        done = False\n        score = 0\n        state = football_env.reset()\n        state  = data_processor.reset(state)\n        while not done:\n            # fresh env\n            global_step += 1\n\n            # get action for the current state and go one step in environment\n            action = agent.get_action(state)\n            \n            next_state, reward, done , _ = football_env.step(action)\n            \n            next_state = data_processor.stack_buffer(next_state)\n            \n            next_action = agent.get_action(next_state)\n            \n            agent.train_model(state, action, reward, next_state, next_action,done)\n            state = next_state\n            # every time step we do training\n            score += reward\n\n            state = copy.deepcopy(next_state)\n\n            if done:\n                scores.append(score)\n                episodes.append(e)\n                print(\"episode:\", e, \"  score:\", score, \"global_step\",\n                      global_step, \"  epsilon:\", agent.epsilon)\n\n        if e % 2 == 0:\n            agent.model.save(\"deep_sarsa.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile main.py\n\n#from kaggle_environments.envs.football.helpers import *\n\n# @human_readable_agent wrapper modifies raw observations \n# provided by the environment:\n# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations\n# into a form easier to work with by humans.\n# Following modifications are applied:\n# - Action, PlayerRole and GameMode enums are introduced.\n# - 'sticky_actions' are turned into a set of active actions (Action enum)\n#    see usage example below.\n# - 'game_mode' is turned into GameMode enum.\n# - 'designated' field is removed, as it always equals to 'active'\n#    when a single player is controlled on the team.\n# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.\n# - Action enum is to be returned by the agent function.\n\n\nimport collections\nimport pickle\nimport zlib\nfrom typing import Tuple, Dict, Any, Union, Callable, List\n\nimport gym\nimport numpy as np\nimport tensorflow as tf\nfrom gfootball.env import observation_preprocessing\nfrom tensorflow import keras\n\nimport random                # Handling random number generation\nimport time                  # Handling time calculation\nfrom skimage import transform# Help us to preprocess the frames\n\nfrom collections import deque# Ordered collection with ends\nfrom collections import namedtuple\nimport numpy as np\n\nstacked_size = 3\n\n\nfrom keras.models import load_model\nimport numpy as np\nimport keras.backend as K\nimport tensorflow as tf\n\n#------------------------------------- Data Preprocessing ---------------------------------------------------------\nclass DataWrapper():\n\n    def __init__(self, obs_shape= (72, 96, 4)) :\n        \"\"\"\n        :param env: Gym env, or None. Allowing None here is unusual,\n                    but we'll reuse the buffer functunality later in\n                    the submission, when we won't be using the gym API.\n        :param obs_shape: Expected shape of single observation.\n        \"\"\"\n        self.data_length = 2\n        self.frame_shape = obs_shape\n        self.init_frame_buffer()\n\n    @staticmethod\n    def normalize_frame(frame):\n        return frame / 255.0\n\n    def init_frame_buffer(self):\n        \"\"\"Create buffer and preallocate with empty arrays of expected shape.\"\"\"\n\n        self.frame_buffer = collections.deque(maxlen=self.data_length)\n\n        for _ in range(self.data_length):\n            self.frame_buffer.append(np.zeros(shape=self.frame_shape))\n\n    def build_buffered_frames(self):\n        \"\"\"\n        Iterate over the last dimenion, and take the difference between this obs \n        and the last obs for each.\n        \"\"\"\n        diff_buff = np.empty(self.frame_shape)\n        for f in range(self.frame_shape[-1]):\n            diff_buff[..., f] = self.frame_buffer[1][..., f] - self.frame_buffer[0][..., f]\n\n        return diff_buff\n\n    def stack_buffer(self, frame ):\n\n        frame = self.normalize_frame(frame)\n        self.frame_buffer.append(frame)\n\n        return self.build_buffered_frames()\n\n    def reset(self , reset_frame):\n        \"\"\"Add initial obs to end of pre-allocated buffer.\n\n        :return: Buffered observation\n        \"\"\"\n        self.init_frame_buffer()\n        self.frame_buffer.append(self.normalize_frame(reset_frame))\n\n        return self.build_buffered_frames()\n\ntry:\n    model_sarsa = load_model('/kaggle_simulations/agent/deep_sarsa.h5')\nexcept (FileNotFoundError, ValueError):\n    model_sarsa = load_model('deep_sarsa.h5')\n\n\ndata_preprocessor = DataWrapper()\n\n\ndef agent(obs):\n\n    # Get the raw observations return by the environment\n    obs = obs['players_raw'][0]\n    # Convert these to the same output as the SMMWrapper we used in training\n    obs = observation_preprocessing.generate_smm([obs]).squeeze()\n    \n    state = data_preprocessor.stack_buffer(obs)\n    \n    #inference the model\n    action_probs = model_sarsa.predict(np.expand_dims(state , 0 ) , steps=1)\n    # Use the SMMFrameProcessWrapper to do the buffering, but not enviroment\n    # stepping or anything related to the Gym API.\n    action = np.argmax(action_probs)\n\n    return [int(action)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Tuple, Dict, List, Any\n\nfrom kaggle_environments import make\n\nenv = make(\"football\", debug=True,configuration={\"save_video\": True,\n                                      \"scenario_name\": \"11_vs_11_kaggle\"})\n\n# Define players\nleft_player = \"/kaggle/working/main.py\"  # A custom agent, eg. random_agent.py or example_agent.py\nright_player = \"run_right\"  # eg. A built in 'AI' agent or the agent again\n\n\noutput: List[Tuple[Dict[str, Any], Dict[str, Any]]] = env.run([left_player, right_player])\n\n#print(f\"Final score: {sum([r['reward'] for r in output[0]])} : {sum([r['reward'] for r in output[1]])}\")\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!tar -czvf submission.tar.gz ./main.py*  ./deep_sarsa.h5*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}